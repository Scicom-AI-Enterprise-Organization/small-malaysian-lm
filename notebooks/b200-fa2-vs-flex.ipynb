{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "048780fa-892c-4196-b60a-0dce4f4dc789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TORCH_LOGS'] = 'recompiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8164bccc-8772-4b5a-b766-062e883d8a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA B200'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1e818f-406f-40e0-8fe8-e428e760b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import flash_attn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.attention.flex_attention import create_block_mask, flex_attention\n",
    "\n",
    "torch._dynamo.config.cache_size_limit = 1000\n",
    "\n",
    "flex_attention = torch.compile(flex_attention, dynamic=False)\n",
    "\n",
    "def generate_list_sum_n(n, length=5, min_val=5):\n",
    "\n",
    "    numbers = [min_val] * length\n",
    "    remaining = n - min_val * length\n",
    "\n",
    "    for _ in range(remaining):\n",
    "        numbers[random.randint(0, length - 1)] += 1\n",
    "\n",
    "    random.shuffle(numbers)\n",
    "    return numbers\n",
    "\n",
    "def causal_mask(b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "\n",
    "def _offsets_to_doc_ids_tensor(offsets):\n",
    "    device = offsets.device\n",
    "    offsets = offsets[offsets != -1]\n",
    "    counts = offsets[1:] - offsets[:-1]\n",
    "    return torch.repeat_interleave(\n",
    "        torch.arange(len(counts), device=device, dtype=torch.int32), counts\n",
    "    )\n",
    "\n",
    "def length_to_offsets(lengths, device):\n",
    "    offsets = [0]\n",
    "    offsets.extend(lengths)\n",
    "    offsets = torch.tensor(offsets, device=device, dtype=torch.int32)\n",
    "    offsets = torch.cumsum(offsets, dim=-1)\n",
    "    return offsets\n",
    "\n",
    "def generate_doc_mask_mod(offsets):\n",
    "    \n",
    "    offsets = pad_sequence(offsets, batch_first = True, padding_value = -1)\n",
    "    docs = [_offsets_to_doc_ids_tensor(offsets[i]) for i in range(offsets.shape[0])]\n",
    "    docs = torch.stack(docs, 0)\n",
    "    \n",
    "    def document_causal_mask(b, h, q_idx, kv_idx):\n",
    "        causal_mask = q_idx >= kv_idx\n",
    "        document_mask = docs[b, q_idx] == docs[b, kv_idx]\n",
    "        return causal_mask & document_mask\n",
    "    \n",
    "    return document_causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629f758c-a59a-495a-9563-4df55856d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4096\n",
    "query_lens = np.array(generate_list_sum_n(sequence_length, length=20, min_val=10), dtype=np.int64)\n",
    "\n",
    "q = torch.randn(1, sequence_length, 32, 128, dtype = torch.bfloat16).cuda()\n",
    "k = torch.randn(1, sequence_length, 32, 128, dtype = torch.bfloat16).cuda()\n",
    "v = torch.randn(1, sequence_length, 32, 128, dtype = torch.bfloat16).cuda()\n",
    "\n",
    "cumsum = [0] + np.cumsum(query_lens).tolist()\n",
    "max_cumsum = int(np.max(cumsum))\n",
    "cu_seq_lens_q = torch.tensor(cumsum, dtype=torch.int32).cuda()\n",
    "max_seqlen_q = np.max(query_lens)\n",
    "\n",
    "out_flash2 = flash_attn.flash_attn_varlen_func(\n",
    "    q = q[0],\n",
    "    k = k[0],\n",
    "    v = v[0],\n",
    "    cu_seqlens_q = cu_seq_lens_q,\n",
    "    cu_seqlens_k = cu_seq_lens_q,\n",
    "    max_seqlen_q = max_seqlen_q,\n",
    "    max_seqlen_k = max_seqlen_q,\n",
    "    causal = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437c3905-e918-40fc-9c97-3f139f3e7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = cu_seq_lens_q\n",
    "seq_len = q.shape[1]\n",
    "device = q.device\n",
    "document_causal_mask = generate_doc_mask_mod(attention_mask[None])\n",
    "attention_mask = create_block_mask(\n",
    "    document_causal_mask, None, None, seq_len, seq_len, device, _compile = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e83d7e19-49e0-4a7d-a776-6b44cf681137",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = flex_attention(\n",
    "    q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), score_mod=None, block_mask=attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acbce269-3bba-4aeb-bc5a-ff3282337524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(out_flash2, o[0].transpose(0, 1), atol=0.125, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa44fa9-59da-40ca-ab12-04bedbb644ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 doc cumsum [0, 304, 592, 921, 1235, 1553, 1854, 2187, 2477, 2807, 3119, 3470, 3767, 4096]\n",
      "0 fa2 0.00041866302490234375\n",
      "0 flex 0.0024042129516601562\n",
      "\n",
      "1 doc cumsum [0, 313, 654, 991, 1323, 1639, 1968, 2277, 2568, 2862, 3155, 3480, 3800, 4096]\n",
      "1 fa2 0.0004990100860595703\n",
      "1 flex 0.0026535987854003906\n",
      "\n",
      "2 doc cumsum [0, 240, 482, 742, 991, 1264, 1540, 1794, 2029, 2283, 2541, 2809, 3058, 3324, 3595, 3845, 4096]\n",
      "2 fa2 0.0005774497985839844\n",
      "2 flex 0.0028772354125976562\n",
      "\n",
      "3 doc cumsum [0, 163, 291, 441, 610, 753, 904, 1048, 1196, 1336, 1484, 1626, 1782, 1929, 2082, 2225, 2365, 2515, 2654, 2815, 2960, 3087, 3227, 3377, 3510, 3655, 3813, 3956, 4096]\n",
      "3 fa2 0.0005552768707275391\n",
      "3 flex 0.0035028457641601562\n",
      "\n",
      "4 doc cumsum [0, 359, 675, 1012, 1374, 1704, 2049, 2391, 2741, 3084, 3388, 3759, 4096]\n",
      "4 fa2 0.00047397613525390625\n",
      "4 flex 0.0030066967010498047\n",
      "\n",
      "5 doc cumsum [0, 164, 328, 479, 643, 815, 980, 1131, 1266, 1421, 1564, 1709, 1860, 1993, 2122, 2269, 2416, 2563, 2729, 2880, 3018, 3181, 3333, 3485, 3645, 3788, 3947, 4096]\n",
      "5 fa2 0.0005409717559814453\n",
      "5 flex 0.0036020278930664062\n",
      "\n",
      "6 doc cumsum [0, 251, 506, 718, 983, 1224, 1442, 1667, 1913, 2192, 2461, 2685, 2917, 3137, 3408, 3646, 3883, 4096]\n",
      "6 fa2 0.0003712177276611328\n",
      "6 flex 0.0022089481353759766\n",
      "\n",
      "7 doc cumsum [0, 319, 602, 934, 1214, 1501, 1778, 2078, 2371, 2674, 2966, 3251, 3518, 3797, 4096]\n",
      "7 fa2 0.0004611015319824219\n",
      "7 flex 0.0026204586029052734\n",
      "\n",
      "8 doc cumsum [0, 192, 384, 574, 747, 945, 1132, 1319, 1515, 1711, 1904, 2062, 2274, 2459, 2649, 2840, 3025, 3206, 3366, 3542, 3718, 3920, 4096]\n",
      "8 fa2 0.0003972053527832031\n",
      "8 flex 0.002355813980102539\n",
      "\n",
      "9 doc cumsum [0, 289, 604, 917, 1210, 1488, 1778, 2067, 2364, 2673, 2956, 3242, 3525, 3819, 4096]\n",
      "9 fa2 0.0003802776336669922\n",
      "9 flex 0.1941392421722412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    query_lens = np.array(generate_list_sum_n(sequence_length, length=random.randint(12, 30), min_val=10), dtype=np.int64)\n",
    "    \n",
    "    q = torch.randn(1, sequence_length, 32, 128, dtype = torch.bfloat16).cuda()\n",
    "    k = torch.randn(1, sequence_length, 32, 128, dtype = torch.bfloat16).cuda()\n",
    "    v = torch.randn(1, sequence_length, 32, 128, dtype = torch.bfloat16).cuda()\n",
    "    \n",
    "    cumsum = [0] + np.cumsum(query_lens).tolist()\n",
    "    print(i, 'doc cumsum', cumsum)\n",
    "    max_cumsum = int(np.max(cumsum))\n",
    "    cu_seq_lens_q = torch.tensor(cumsum, dtype=torch.int32).cuda()\n",
    "    max_seqlen_q = np.max(query_lens)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "\n",
    "    out_flash2 = flash_attn.flash_attn_varlen_func(\n",
    "        q = q[0],\n",
    "        k = k[0],\n",
    "        v = v[0],\n",
    "        cu_seqlens_q = cu_seq_lens_q,\n",
    "        cu_seqlens_k = cu_seq_lens_q,\n",
    "        max_seqlen_q = max_seqlen_q,\n",
    "        max_seqlen_k = max_seqlen_q,\n",
    "        causal = True\n",
    "    )\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "\n",
    "    print(i, 'fa2', dt)\n",
    "\n",
    "    attention_mask = cu_seq_lens_q\n",
    "    seq_len = q.shape[1]\n",
    "    device = q.device\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    document_causal_mask = generate_doc_mask_mod(attention_mask[None])\n",
    "    attention_mask = create_block_mask(\n",
    "        document_causal_mask, None, None, seq_len, seq_len, device, _compile = True)\n",
    "\n",
    "    o = flex_attention(\n",
    "        q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), score_mod=None, block_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "\n",
    "    print(i, 'flex', dt)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
