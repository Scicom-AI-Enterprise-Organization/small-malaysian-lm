{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22835bba-6768-48f7-abde-7c0346389149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supported flash-attn versions are >= 2.7.3, <= 2.8.1. Found flash-attn 2.8.3.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen3ForCausalLM\n",
    "from transformer_engine import pytorch as te\n",
    "from transformer_engine.common import recipe\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def convert_model(model):\n",
    "    \"\"\"\n",
    "    Recursively converts the linear and layernorm layers of a model to their `transformers_engine` counterpart.\n",
    "    Modified from https://github.com/huggingface/accelerate/blob/main/src/accelerate/utils/transformer_engine.py#L26\n",
    "    Should apply after load the model with intended precision.\n",
    "    \"\"\"\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "\n",
    "        if \"lm_head\" in name:\n",
    "            continue\n",
    "        if isinstance(module, nn.Linear):\n",
    "            has_bias = module.bias is not None\n",
    "            params_to_gather = [module.weight]\n",
    "            if any(p % 16 != 0 for p in module.weight.shape):\n",
    "                return\n",
    "            te_module = te.Linear(\n",
    "                module.in_features, module.out_features, bias=has_bias, params_dtype=module.weight.dtype\n",
    "            )\n",
    "            te_module.weight.copy_(module.weight)\n",
    "            if has_bias:\n",
    "                te_module.bias.copy_(module.bias)\n",
    "\n",
    "            setattr(model, name, te_module)\n",
    "        else:\n",
    "            convert_model(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5096b6-5b41-4b95-90af-f20c8770262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Qwen3ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def estimate_flops(self, t = 4096):\n",
    "        \"\"\"\n",
    "        Return the estimated FLOPs per token for the model. Ref: https://arxiv.org/abs/2204.02311 \n",
    "        Borrow from https://github.com/karpathy/nanochat/blob/master/nanochat/gpt.py#L220\n",
    "        \"\"\"\n",
    "        nparams = sum(p.numel() for p in self.parameters())\n",
    "        nparams_embedding = self.model.embed_tokens.weight.numel()\n",
    "        l, h, q = self.config.num_hidden_layers, self.config.num_attention_heads, self.config.head_dim\n",
    "        num_flops_per_token = 6 * (nparams - nparams_embedding) + 12 * l * h * q * t\n",
    "        return num_flops_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52589242-49c9-4289-b42d-e05f7ba71368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5461377024"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model.from_pretrained('Qwen/Qwen3-0.6B-Base')\n",
    "model.estimate_flops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623c4d7c-dc4b-48ec-b500-05522f2a817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    convert_model(model)\n",
    "\n",
    "_ = model.to('cuda')\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619a677f-4fe2-4efa-bcaf-8f79ec07cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(model.parameters(), lr=2e-5, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89f8dec-d214-4d47-ba97-13d6465eee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp8_recipe = recipe.DelayedScaling(margin=0, fp8_format=recipe.Format.E4M3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bab138f-47a9-4fce-88e4-cc57adb5d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must divisible by 8\n",
    "input_ids = torch.arange(8)[None].cuda()\n",
    "labels = input_ids.clone().cuda()\n",
    "b = {\n",
    "    'input_ids': input_ids,\n",
    "    'labels': labels,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4acae2b0-defb-47db-a347-42162ab5f248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8]), torch.Size([1, 8]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec65cf5f-b975-4570-b0a6-55958dc84d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1018 08:26:15.764000 6077 site-packages/torch/_dynamo/convert_frame.py:1016] [13/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W1018 08:26:15.764000 6077 site-packages/torch/_dynamo/convert_frame.py:1016] [13/8]    function: 'torch_dynamo_resume_in_forward_at_202' (/venv/main/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:202)\n",
      "W1018 08:26:15.764000 6077 site-packages/torch/_dynamo/convert_frame.py:1016] [13/8]    last reason: 13/7: past_key_values.layers[7].is_initialized == False      \n",
      "W1018 08:26:15.764000 6077 site-packages/torch/_dynamo/convert_frame.py:1016] [13/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1018 08:26:15.764000 6077 site-packages/torch/_dynamo/convert_frame.py:1016] [13/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    }
   ],
   "source": [
    "with torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=True):\n",
    "    if fp8_recipe is not None:\n",
    "        with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "            out = model(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cea12e5-ab4b-4211-8210-a5b5afa5c739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8944, device='cuda:0', grad_fn=<CompiledFunctionBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c2366c2-06da-4195-99d8-598f6a0b48c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = out[\"loss\"]\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f2ab1-fdbc-4baf-a108-c7cb4dfbdc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "optim.step()\n",
    "scheduler.step()\n",
    "optim.zero_grad(set_to_none=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
