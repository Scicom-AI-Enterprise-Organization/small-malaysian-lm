{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d53093e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from streaming import MDSWriter\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from streaming import LocalDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from multiprocess import Pool\n",
    "import itertools\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield (l[i: i + n], i // n)\n",
    "\n",
    "def multiprocessing(strings, function, cores=6, returned=True):\n",
    "    df_split = chunks(strings, len(strings) // cores)\n",
    "    pool = Pool(cores)\n",
    "    pooled = pool.map(function, df_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    if returned:\n",
    "        return list(itertools.chain(*pooled))\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "columns = {\n",
    "    'input_ids': 'uint32',\n",
    "    'position_ids': 'uint32',\n",
    "    'attention_mask': 'uint32',\n",
    "}\n",
    "hashes = 'sha1', 'xxh64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ffc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-1.7B-Base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db5dec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/malaysia-ai/pretrain-text-dataset/resolve/main/wikipedia-2023-10-01.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e06d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438316 wikipedia-2023-10-01.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l wikipedia-2023-10-01.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e78796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "438316it [00:01, 284358.71it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "with open('wikipedia-2023-10-01.jsonl') as fopen:\n",
    "    for l in tqdm(fopen):\n",
    "        l = json.loads(l)\n",
    "        if len(l) > 1:\n",
    "            texts.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a510876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(batch, batch_position_ids):\n",
    "    input_ids = []\n",
    "    position_ids = []\n",
    "    masks = []\n",
    "    for i in range(len(batch)):\n",
    "        l = len(batch[i])\n",
    "        input_ids.extend(batch[i])\n",
    "        position_ids.extend(batch_position_ids[i])\n",
    "        masks.append(l)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(input_ids).astype(np.uint32),\n",
    "        'position_ids': np.array(position_ids).astype(np.uint32),\n",
    "        'attention_mask': np.array(masks).astype(np.uint32),\n",
    "    }\n",
    "\n",
    "def slice_and_balance(nested_list, size):\n",
    "    first = []\n",
    "    balance = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sublist in nested_list:\n",
    "        if current_size < size:\n",
    "            remaining_space = size - current_size\n",
    "            if len(sublist) <= remaining_space:\n",
    "                first.append(sublist)\n",
    "                current_size += len(sublist)\n",
    "            else:\n",
    "                first.append(sublist[:remaining_space])\n",
    "                balance.append(sublist[remaining_space:])\n",
    "                current_size = size\n",
    "        else:\n",
    "            balance.append(sublist)\n",
    "    \n",
    "    return first, balance\n",
    "\n",
    "def loop(rows, block_size = 4096, folder = 'tokenized-4k-qwen'):\n",
    "    rows, index = rows\n",
    "    out_root = f'{folder}/tokenized-{index}'\n",
    "    os.system(f'rm -rf {out_root}')\n",
    "    count = 0\n",
    "    temp = []\n",
    "    position_ids = []\n",
    "    last_block, last_position_block = None, None\n",
    "    with MDSWriter(out=out_root, columns=columns, compression=None, hashes=hashes) as out:\n",
    "        for row in tqdm(rows):\n",
    "            outputs = tokenizer(row, add_special_tokens = False)\n",
    "            temp.append(outputs['input_ids'])\n",
    "            position_ids.append(range(len(outputs['input_ids'])))\n",
    "            count += len(outputs['input_ids'])\n",
    "            while count >= block_size:\n",
    "                block, temp = slice_and_balance(temp, block_size)\n",
    "                block_position, position_ids = slice_and_balance(position_ids, block_size)\n",
    "                count = count - block_size\n",
    "                o = collator(block, block_position)\n",
    "                last_block = block\n",
    "                last_position_block = block_position\n",
    "                out.write(o)\n",
    "                \n",
    "        block, _ = slice_and_balance(last_block, block_size - count)\n",
    "        block_position, _ = slice_and_balance(last_position_block, block_size - count)\n",
    "\n",
    "        block.extend(temp)\n",
    "        block_position.extend(position_ids)\n",
    "\n",
    "        o = collator(block, block_position)\n",
    "        if len(o['input_ids']) == block_size:\n",
    "            out.write(o)\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ec3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop((texts, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aecbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = multiprocessing(texts, loop, cores = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff67b927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenized-4k-qwen/tokenized-0',\n",
       " 'tokenized-4k-qwen/tokenized-1',\n",
       " 'tokenized-4k-qwen/tokenized-2',\n",
       " 'tokenized-4k-qwen/tokenized-3',\n",
       " 'tokenized-4k-qwen/tokenized-4',\n",
       " 'tokenized-4k-qwen/tokenized-5',\n",
       " 'tokenized-4k-qwen/tokenized-6',\n",
       " 'tokenized-4k-qwen/tokenized-7',\n",
       " 'tokenized-4k-qwen/tokenized-8',\n",
       " 'tokenized-4k-qwen/tokenized-9',\n",
       " 'tokenized-4k-qwen/tokenized-10',\n",
       " 'tokenized-4k-qwen/tokenized-11',\n",
       " 'tokenized-4k-qwen/tokenized-12',\n",
       " 'tokenized-4k-qwen/tokenized-13',\n",
       " 'tokenized-4k-qwen/tokenized-14',\n",
       " 'tokenized-4k-qwen/tokenized-15',\n",
       " 'tokenized-4k-qwen/tokenized-16',\n",
       " 'tokenized-4k-qwen/tokenized-17',\n",
       " 'tokenized-4k-qwen/tokenized-18',\n",
       " 'tokenized-4k-qwen/tokenized-19',\n",
       " 'tokenized-4k-qwen/tokenized-20']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = sorted(glob('tokenized-4k-qwen/tokenized-*'), key = lambda x: int(x.split('-')[-1]))\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33bb2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf multipacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f56f71a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3538/3538 [00:00<00:00, 10516.82it/s]\n",
      "100%|██████████| 2459/2459 [00:00<00:00, 10172.88it/s]\n",
      "100%|██████████| 943/943 [00:00<00:00, 5555.07it/s]\n",
      "100%|██████████| 1168/1168 [00:00<00:00, 20330.62it/s]\n",
      "100%|██████████| 323/323 [00:00<00:00, 2347.84it/s]\n",
      "100%|██████████| 604/604 [00:00<00:00, 19859.67it/s]\n",
      "100%|██████████| 1095/1095 [00:00<00:00, 20381.39it/s]\n",
      "100%|██████████| 673/673 [00:00<00:00, 4219.48it/s]\n",
      "100%|██████████| 291/291 [00:00<00:00, 20265.03it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 20451.89it/s]\n",
      "100%|██████████| 235/235 [00:00<00:00, 19258.35it/s]\n",
      "100%|██████████| 357/357 [00:00<00:00, 19947.86it/s]\n",
      "100%|██████████| 524/524 [00:00<00:00, 3537.77it/s]\n",
      "100%|██████████| 269/269 [00:00<00:00, 20316.70it/s]\n",
      "100%|██████████| 976/976 [00:00<00:00, 20661.49it/s]\n",
      "100%|██████████| 1360/1360 [00:00<00:00, 7347.94it/s]\n",
      "100%|██████████| 941/941 [00:00<00:00, 20656.62it/s]\n",
      "100%|██████████| 1832/1832 [00:00<00:00, 8782.87it/s]\n",
      "100%|██████████| 1869/1869 [00:00<00:00, 8894.00it/s]\n",
      "100%|██████████| 2475/2475 [00:00<00:00, 10391.89it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 12520.31it/s]\n"
     ]
    }
   ],
   "source": [
    "with MDSWriter(out='multipacking', columns=columns, compression=None, hashes=hashes) as out:\n",
    "    for f in folders:\n",
    "        try:\n",
    "            dataset = LocalDataset(local=f)\n",
    "            for i in tqdm(range(len(dataset))):\n",
    "                out.write(dataset[i])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33f4ad07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22206"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = LocalDataset('multipacking')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90c8c4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': array([ 325,  302,   45, 1377,  562,   74,  516,  538,   87,  270],\n",
       "       dtype=uint32),\n",
       " 'input_ids': array([57635,  5103,   300, ...,    17,    11, 10371], dtype=uint32),\n",
       " 'position_ids': array([1483, 1484, 1485, ...,  267,  268,  269], dtype=uint32)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02623031",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf upload Scicom-intl/mosaic-ms-wikipedia-2023-10-01 multipacking --repo-type=dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-whisper",
   "language": "python",
   "name": "finetune-whisper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
